{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task15NPL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frglAbYa9zz9"
      },
      "source": [
        "**Name :Suraj Sanjeev Pawar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGeWWhdQ93Dh"
      },
      "source": [
        "**Registration ID :GO-WTP-1839**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66Df_xHe96Xd"
      },
      "source": [
        "**Task-15 : Natural Language Processing**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NfTOCJY-C3S"
      },
      "source": [
        "***This task is given by Goeduhub Technologies as Online Winter training in Machine Learning using Python.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJd_3TVmdagK"
      },
      "source": [
        "***Natural Language Processing(NLP) is the study of computational treatment of human language. NLP is a part of Artificial Intelligence. In this module, its use and terms related to NLP are focused and described.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg2GXnpseDgl"
      },
      "source": [
        "***NLP is the study of computational treatment of natural (Human) language. NLP is a part of Artificial Intelligence.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ragCpHiNfaQM"
      },
      "source": [
        "***NLTK is a leading platform for building Python programs to work with human language data.NLTK, the most widely-mentioned NLP library for Python.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFeSD0DSe8KI"
      },
      "source": [
        "###Tokenizing\r\n",
        "***Breaking a text based on tokens or into tokens.\r\n",
        "word tokenizing - Separate by words , sentence tokenizing- separate by sentences.Corpora (Corpus-singular)- Body of a text. , Lexicon- Meaning of a word (changes in meaning of a word depends on situation)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxAWb-Scc-m9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "003b1d27-a147-44bf-e36b-fb6118a515aa"
      },
      "source": [
        "#Importing libraries \r\n",
        "import nltk\r\n",
        "import nltk.corpus\r\n",
        "from nltk.corpus import gutenberg\r\n",
        "\r\n",
        "#Accessing a text corpora\r\n",
        "nltk.download('gutenberg')\r\n",
        "print(nltk.corpus.gutenberg.fileids())\r\n",
        "\r\n",
        "#Accessing a particular text file\r\n",
        "bible= nltk.corpus.gutenberg.words(\"bible-kjv.txt\")\r\n",
        "bible"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'The', 'King', 'James', 'Bible', ']', 'The', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0xhJ5lEf3kI",
        "outputId": "4a3792a6-06d5-4457-cf9b-8effde6048ed"
      },
      "source": [
        "emma= nltk.corpus.gutenberg.words(\"austen-emma.txt\")\r\n",
        "len(emma)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ4j47CBeh2U",
        "outputId": "1ee560eb-60ea-46ad-c383-2da619a2f7d5"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize , word_tokenize\r\n",
        "#Downloading 'punkt'\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "#Creating a text to see meaning of tokenizing\r\n",
        "text=\"We provide Summer, winter, regular training in Artificial Intelligence(AI). Machine Learning(ML), Deep Learning(DL) at GOeduhub technologies\"\r\n",
        "print(sent_tokenize(text))\r\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['We provide Summer, winter, regular training in Artificial Intelligence(AI).', 'Machine Learning(ML), Deep Learning(DL) at GOeduhub technologies']\n",
            "['We', 'provide', 'Summer', ',', 'winter', ',', 'regular', 'training', 'in', 'Artificial', 'Intelligence', '(', 'AI', ')', '.', 'Machine', 'Learning', '(', 'ML', ')', ',', 'Deep', 'Learning', '(', 'DL', ')', 'at', 'GOeduhub', 'technologies']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HECeSS4g2kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8b7aee-e845-43ae-9094-4cbc9b2e8a56"
      },
      "source": [
        "\r\n",
        "#Downloading all packages together \r\n",
        "nltk.download()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: l\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [*] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: l\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: l\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [*] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: q\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J7FAWhHhFbp"
      },
      "source": [
        "###Stemming\r\n",
        "***Its kind of form of data preprocessing .Normalizing a word into its base form or root form.But why we need to do stemming of words.***\r\n",
        "\r\n",
        "***Lets's take an example\r\n",
        "sentence1- I was taking a ride in car.\r\n",
        "sentence2-I was riding in car.***\r\n",
        "\r\n",
        "***As we know the meaning of both sentences is same, but see the word ride.\r\n",
        "We use the same verb in both sentences but in a different way.\r\n",
        "The use of Suffix and Prefix in the root word creates many words from the \r\n",
        "word, which will make our database very large and difficult to handle.***\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmDf6MKxeqdI",
        "outputId": "494ef364-941c-4f0e-987b-38fe20f9586a"
      },
      "source": [
        "#PorterStemmer\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "ps=PorterStemmer()\r\n",
        "test= ['Accept','Accepted','accepting','acceptation','acceptoid']\r\n",
        "print(\"PorterStemmer Result\")\r\n",
        "for i in test:\r\n",
        "    w=ps.stem(i)\r\n",
        "    print(w)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PorterStemmer Result\n",
            "accept\n",
            "accept\n",
            "accept\n",
            "accept\n",
            "acceptoid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQnEQu5liQJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021fd28b-897d-4a89-e87c-ecb11b64e091"
      },
      "source": [
        "#LancasterStemmer \r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "ls=LancasterStemmer()\r\n",
        "print(\"LancasterStemmer Result\")\r\n",
        "for i in test:\r\n",
        "    w=ls.stem(i)\r\n",
        "    print(w)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LancasterStemmer Result\n",
            "acceiv\n",
            "acceiv\n",
            "acceiv\n",
            "acceiv\n",
            "acceptoid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1zgSfzFhtJp"
      },
      "source": [
        "###Lemmatizing\r\n",
        "***Lemmatizing is similar to the stemming , In lemmatizing we basically groups together different form of a root word. But the output of lemmatizing is a proper word not like stemming.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En0aWrzOetgD",
        "outputId": "c70ad237-bfec-462b-f758-f1b239f3e429"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "#Defining Lemmatizer \r\n",
        "lem=WordNetLemmatizer()\r\n",
        "print(lem.lemmatize(\"dogs\"))\r\n",
        "print(lem.lemmatize(\"riding\"))\r\n",
        "print(lem.lemmatize(\"cacti\"))\r\n",
        "\r\n",
        "#Printing without defining pos\r\n",
        "print(lem.lemmatize(\"better\"))\r\n",
        "\r\n",
        "#Printing with Pos (Part of speech)\r\n",
        "print(lem.lemmatize(\"better\", pos=\"a\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "dog\n",
            "riding\n",
            "cactus\n",
            "better\n",
            "good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yXpm6fAhz00"
      },
      "source": [
        "###Stopword \r\n",
        "***As we know there are several words in English, Which are necessary to make a sentence .for example: of, the etc...But these words are not useful in NLP. This type of words are known as stopword.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AlOoveVezXj",
        "outputId": "39db5819-3023-4d68-e97f-5d0ae57ceede"
      },
      "source": [
        "from nltk.tokenize import  word_tokenize\r\n",
        "from nltk.corpus import  stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "text=\"Python is a programming language for multiple purpose\"\r\n",
        "sw=set(stopwords.words(\"english\"))\r\n",
        "\r\n",
        "#Printing Stopwords in English language \r\n",
        "print(sw)\r\n",
        "words= word_tokenize(text)\r\n",
        "ntext= []\r\n",
        "for i in words:\r\n",
        "    if i not in sw:\r\n",
        "        ntext.append(i)\r\n",
        "\r\n",
        "#Printing our text after avoiding stopwords \r\n",
        "print(ntext)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{'they', 'between', \"you'd\", 'should', 'not', 'are', 'up', 'be', 'mustn', 'do', 'own', 'further', 'again', 'its', 'these', 'wasn', \"couldn't\", 'through', 'more', 'why', 'was', 'most', 'him', 'as', 'than', 'have', 'whom', 't', 'few', 'mightn', 'too', 'where', 'just', 'doing', 've', 'yours', 'ain', 'so', 'needn', 'hadn', 'both', 'm', \"should've\", 'didn', 'won', 'ma', 'each', 'ourselves', 'does', 'll', \"won't\", 'same', 'theirs', 'o', 'yourselves', 'above', 'themselves', 'off', 'myself', 'haven', 'their', 'being', 'this', 'your', 'that', 'other', 'and', 'if', 'can', 'before', 'there', 'himself', 'after', \"wasn't\", 'about', 'couldn', \"shouldn't\", 'we', 'his', 'to', 're', 'with', 'any', 'did', 'itself', 'here', 'don', 'out', 'or', 'the', 'by', 'd', 'against', \"didn't\", 'weren', 'in', \"you'll\", 'having', 'she', 'on', \"haven't\", \"don't\", \"hasn't\", 'of', 'yourself', 'what', 'but', 'when', 'no', \"you've\", 'he', 'at', 'some', 'below', 'those', \"mustn't\", 'our', \"she's\", \"it's\", \"you're\", 'once', 'very', \"needn't\", 'all', 'into', 's', 'i', 'from', 'while', 'me', 'had', 'nor', 'only', 'y', \"hadn't\", \"aren't\", 'doesn', 'because', 'it', 'during', 'then', 'hers', 'who', \"wouldn't\", 'an', 'her', \"shan't\", 'herself', 'which', 'will', 'isn', 'over', 'ours', 'for', 'were', 'aren', 'is', 'until', 'you', 'down', 'has', 'my', 'hasn', 'shan', 'a', 'wouldn', 'shouldn', 'under', \"mightn't\", 'been', \"isn't\", \"doesn't\", 'how', \"that'll\", 'am', \"weren't\", 'now', 'such', 'them'}\n",
            "['Python', 'programming', 'language', 'multiple', 'purpose']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp_N-h5ekkgy"
      },
      "source": [
        "###Part of Speech (POS tagging)\r\n",
        "***There are eight parts of speech in the English language: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection.***\r\n",
        "\r\n",
        "***And we also know that a word can behave both as a verb, noun and any pos , according to the situation of word in sentence.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU1P_pgQOmdU",
        "outputId": "b8cdde68-91a6-483a-fb90-b16bc104abe5"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkDFqpz8kmND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ef37df-fa49-4b2a-8b4c-48a59ee1374c"
      },
      "source": [
        "from nltk.tokenize import  word_tokenize\r\n",
        "text=\"Python is a programming language for multiple purpose\"\r\n",
        "#Tokenizing by word \r\n",
        "words= word_tokenize(text)\r\n",
        "#Passing through postaglist \r\n",
        "for i in words:\r\n",
        "    print(nltk.pos_tag([i]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Python', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('a', 'DT')]\n",
            "[('programming', 'VBG')]\n",
            "[('language', 'NN')]\n",
            "[('for', 'IN')]\n",
            "[('multiple', 'NN')]\n",
            "[('purpose', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFMAE1Jqkrjm"
      },
      "source": [
        "###Named Entity Recognition (NER)\r\n",
        "***The process of detecting name of a any entity (a person name, organization name, place name etc...***\r\n",
        "\r\n",
        "***It is basically a classification process ,In which we classify the noun which we get from passing a text through pos (part of speech). We will categorize different noun into their respective categories like Person name , Location name etc...***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVRG_lSIOvDo",
        "outputId": "3bbee0fa-0a7a-4b74-f1e6-20625cc62574"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('maxent_ne_chunker')\r\n",
        "nltk.download('words')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9BbE_ZBkrQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a5f276-a79a-40b9-aab2-ed889d9faa12"
      },
      "source": [
        "#importing important libraries and module \r\n",
        "from nltk.tokenize import  word_tokenize\r\n",
        "#ne_chunk work as ner (named entity recognition)\r\n",
        "from nltk import  ne_chunk \r\n",
        "text=\"Mark and John are working at Google.\"\r\n",
        "#Tokenizing by words\r\n",
        "words= word_tokenize(text)\r\n",
        "\r\n",
        "#Passing through POS \r\n",
        "N_word=nltk.pos_tag(words)\r\n",
        "\r\n",
        "#Chunking of words \r\n",
        "F_word=ne_chunk(N_word)\r\n",
        "print(F_word)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Mark/NNP)\n",
            "  and/CC\n",
            "  (PERSON John/NNP)\n",
            "  are/VBP\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Google/NNP)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq3NpVN5kzrE"
      },
      "source": [
        "###Syntax Tree /Parse Tree\r\n",
        "***A parse tree or parsing tree or derivation tree or concrete syntax tree is an ordered, rooted tree that represents the syntactic structure of a string (text) according to some context-free grammar (according grammar rules).***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekDAKQZskyzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8c4865-b320-4754-8d33-06d9ca6183e9"
      },
      "source": [
        "import nltk \r\n",
        "from nltk.tokenize import  word_tokenize\r\n",
        "#Import Chunking parser\r\n",
        "from nltk.tokenize import RegexpTokenizer \r\n",
        "#from nltk import ne_chunk\r\n",
        "sentence = \"The little mouse ate the fresh cheeze.\"\r\n",
        "#Defining NP grammar \r\n",
        "grammar = ('''\r\n",
        "    NP: {<DT>?<JJ>*<NN>} \r\n",
        "    ''')\r\n",
        "#creating chunk parser \r\n",
        "chunkParser = nltk.RegexpParser(grammar)\r\n",
        "tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\r\n",
        "#Creating a syntax tree\r\n",
        "tree = chunkParser.parse(tagged)\r\n",
        "print(tree)\r\n",
        "#Drawing a tree\r\n",
        "#tree.draw()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP The/DT little/JJ mouse/NN)\n",
            "  ate/VB\n",
            "  (NP the/DT fresh/JJ cheeze/NN)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyHbXcoXqcAa"
      },
      "source": [
        "***Thank you Goeduhub Techonologies***"
      ]
    }
  ]
}